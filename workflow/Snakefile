import os
import pandas as pd
import yaml
from scripts.utils import settings
from snakemake.utils import validate

env_configs = settings.env_configs

NEO4J_IMPORTDIR = env_configs['neo4j_import_dir']
NEO4J_HTTP = env_configs['graph_http']
NEO4J_ADDRESS = env_configs['neo4j_address']

NEO4J_LOGDIR = env_configs['neo4j_log_dir']
PROCESSINGDIR = env_configs['processing_dir'].replace('/','.')
SNAKEMAKELOGS = env_configs['snakemake_logs']
CONTAINER_NAME = env_configs["container_name"]
CONFIG_PATH = env_configs["config_path"]

NODEDIR = 'nodes'
RELDIR = 'rels'

THREADS=env_configs['threads']

#set main config param to read data_integration.yaml
DATA_CONFIG = os.path.join(CONFIG_PATH,"data_integration.yaml")
DATA_CONFIG_SCHEMA = "config/data_integration.schema.yaml"
DB_SCHEMA = os.path.join(CONFIG_PATH,"db_schema.yaml")
DB_SCHEMA_NODES_SCHEMA = "config/db_schema_nodes.schema.yaml"
DB_SCHEMA_RELS_SCHEMA = "config/db_schema_rels.schema.yaml"

configfile: DATA_CONFIG

#validata config files
rule check_configs:
    log: f"{SNAKEMAKELOGS}/check_configs.log"
    input: 
        DATA_CONFIG, 
        DB_SCHEMA
    run:
        #validata data integration config file
        if NODEDIR in config:
            nodes = config[NODEDIR]
            for i in nodes:
                validate(nodes[i], os.path.join(os.getcwd(),DATA_CONFIG_SCHEMA))
        if RELDIR in config:
            rels = config[RELDIR]
            for i in rels:
                validate(rels[i], os.path.join(os.getcwd(),DATA_CONFIG_SCHEMA))

        #validate db schema config file
        with open(os.path.join(CONFIG_PATH,"db_schema.yaml")) as file:
            db_schema = yaml.load(file,Loader=yaml.FullLoader)
            if 'meta_nodes' in db_schema:
                nodes = db_schema['meta_nodes']
                for i in nodes:
                    validate(nodes[i], os.path.join(os.getcwd(),DB_SCHEMA_NODES_SCHEMA))
            else:
                print('The db schame has no nodes!')
                exit()
            if 'meta_rels' in db_schema:
                rels = db_schema['meta_rels']
                for i in rels:
                    validate(rels[i], os.path.join(os.getcwd(),DB_SCHEMA_RELS_SCHEMA))

rule all:
    input: f"{SNAKEMAKELOGS}/import_report.log"    

#remove all intermediate files
rule clean_all:
    log: f"{SNAKEMAKELOGS}/clean_all.log"
    params:
        NEO4J_IMPORTDIR = NEO4J_IMPORTDIR,
        NODEDIR = NODEDIR,
        RELDIR = RELDIR,
        SNAKEMAKELOGS = SNAKEMAKELOGS
    shell:
        """
        echo 'Deleting {params.NEO4J_IMPORTDIR}/{params.NODEDIR}/merged/*'
        rm -f {params.NEO4J_IMPORTDIR}/{params.NODEDIR}/merged/*
        echo 'Deleting {params.NEO4J_IMPORTDIR}/{params.NODEDIR}/created.txt'
        rm -f {params.NEO4J_IMPORTDIR}/{params.NODEDIR}/created.txt
        echo 'Deleting {params.NEO4J_IMPORTDIR}/{params.RELDIR}/created.txt'
        rm -f {params.NEO4J_IMPORTDIR}/{params.RELDIR}/created.txt
        echo 'Deleting find {params.SNAKEMAKELOGS}/{params.NODEDIR} -name "*.log" -delete'
        if [ -f {params.SNAKEMAKELOGS}/{params.NODEDIR} ]; then find {params.SNAKEMAKELOGS}/{params.NODEDIR} -name "*.log" -delete; fi
        echo 'Deleting find {params.SNAKEMAKELOGS}/{params.RELDIR} -name "*.log" -delete'
        if [ -f {params.SNAKEMAKELOGS}/{params.RELDIR} ]; then find {params.SNAKEMAKELOGS}/{params.RELDIR} -name "*.log" -delete; fi
        echo 'Deleting {params.NEO4J_IMPORTDIR}/master*'
        rm -f {params.NEO4J_IMPORTDIR}/master*
        echo 'Deleting {params.SNAKEMAKELOGS}/master*'
        rm -f {params.SNAKEMAKELOGS}/master*
        echo 'Deleting {params.NEO4J_IMPORTDIR}/logs/*'
        rm -f {params.NEO4J_IMPORTDIR}/logs/*
        #not sure if below is too severe
        echo 'Deleting find {params.NEO4J_IMPORTDIR}/{params.NODEDIR} -name "*.csv.gz" -delete -o -name "*import-nodes.txt" -delete'
        if [ -d {params.NEO4J_IMPORTDIR}/{params.NODEDIR} ]; 
            then find {params.NEO4J_IMPORTDIR}/{params.NODEDIR} -name "*.csv.gz" -delete -o -name "*import-nodes.txt" -delete; 
        else
            echo "{params.NEO4J_IMPORTDIR}/{params.NODEDIR} is missing"
        fi
        echo 'Deleting find {params.NEO4J_IMPORTDIR}/{params.RELDIR} -name "*.csv.gz" -delete -o -name "*import-nodes.txt" -delete'
        if [ -d {params.NEO4J_IMPORTDIR}/{params.RELDIR} ]; 
            then find {params.NEO4J_IMPORTDIR}/{params.RELDIR} -name "*.csv.gz" -delete -o -name "*import-nodes.txt" -delete; 
        else
            echo "{params.NEO4J_IMPORTDIR}/{params.RELDIR} is missing"
        fi
        """

#remove all intermediate files
rule clean_for_build:
    log: f"{SNAKEMAKELOGS}/clean_for_build.log"
    params:
        NEO4J_IMPORTDIR = NEO4J_IMPORTDIR,
        NODEDIR = NODEDIR,
        RELDIR = RELDIR,
        SNAKEMAKELOGS = SNAKEMAKELOGS
    shell:
        """
        rm -f {params.NEO4J_IMPORTDIR}/{params.NODEDIR}/merged/*
        rm -f {params.NEO4J_IMPORTDIR}/{params.NODEDIR}/created.txt
        rm -f {params.NEO4J_IMPORTDIR}/{params.RELDIR}/created.txt
        rm -f {params.NEO4J_IMPORTDIR}/master*
        rm -f {params.SNAKEMAKELOGS}/master*
        """       

rule check_new_data:
    log: f"{SNAKEMAKELOGS}/check_new_data.log"
    input: 
        expand(os.path.join(NEO4J_IMPORTDIR,NODEDIR,'{node}','{node}.csv.gz'), node = config[NODEDIR]),
        expand(os.path.join(NEO4J_IMPORTDIR,RELDIR,'{rel}','{rel}.csv.gz'), rel = config[RELDIR])

rule create_graph:
    log: graph=f"{SNAKEMAKELOGS}/create_graph.log", build=f"{SNAKEMAKELOGS}/build_graph.log", constraints=f"{SNAKEMAKELOGS}/master_constraints.log"
    input: f"{NEO4J_IMPORTDIR}/master_import.sh"
    output: f"{SNAKEMAKELOGS}/import_report.log"
    shell:
        """
        echo 'Starting database...'
        #force load of .env file if it exists to avoid docker issues with cached variables
        if [ -f .env ]; then export $(cat .env | sed 's/#.*//g' | xargs); fi
        #create neo4j directories if not already done
        echo 'Creating Neo4j graph directories'
        python -m workflow.scripts.graph_build.create_neo4j > {log.graph} 2> {log.graph}
        #create container
        docker-compose up -d 
        #docker-compose up -d --no-recreate 
        echo 'removing old database...'
        docker exec --user neo4j {CONTAINER_NAME} sh -c 'rm -rf /var/lib/neo4j/data/databases/neo4j' > {log.graph} 2> {log.graph}
        docker exec --user neo4j {CONTAINER_NAME} sh -c 'rm -f /var/lib/neo4j/data/transactions/neo4j/*' > {log.graph} 2> {log.graph}
        echo 'running import...'
        SECONDS=0
        docker exec --user neo4j {CONTAINER_NAME} sh /var/lib/neo4j/import/master_import.sh > {log.build} 2> {log.build}
        duration=$SECONDS
        echo "Import took $(($duration / 60)) minutes and $(($duration % 60)) seconds."
        echo 'stopping container {CONTAINER_NAME}...'
        docker stop {CONTAINER_NAME}
        echo 'starting container {CONTAINER_NAME}...'
        docker start {CONTAINER_NAME}
        echo 'waiting a bit...'
        sleep 30
        echo 'adding contraints and extra bits...'
        docker exec --user neo4j {CONTAINER_NAME} sh /var/lib/neo4j/import/master_constraints.sh > {log.constraints} 2> {log.constraints}
        echo 'waiting a bit for indexes to populate...'
        sleep 30
        echo 'checking import report...'
        python -m workflow.scripts.graph_build.import-report-check {NEO4J_LOGDIR}/import.report > {output}
        echo 'Neo4j browser available here: http://{NEO4J_ADDRESS}:{NEO4J_HTTP}/browser'
        #open http://{NEO4J_ADDRESS}:{NEO4J_HTTP}/browser
        """

rule prepare_for_load:
    log: f"{SNAKEMAKELOGS}/prepare_for_load.log"
    input: 
        os.path.join(NEO4J_IMPORTDIR,NODEDIR,'created.txt'),
        os.path.join(NEO4J_IMPORTDIR,RELDIR,'created.txt')
    output: f"{NEO4J_IMPORTDIR}/master_import.sh"
    shell: 
        """
        rm -f {NEO4J_IMPORTDIR}/{NODEDIR}/merged/*
        python -m workflow.scripts.graph_build.merge_sources > {log} 2> {log}
        python -m workflow.scripts.graph_build.create_master_import > {log} 2> {log}
        """

rule read_config_nodes:
    """
    Get the node information from the config file
    """
    #threads: workflow.cores * 0.75
    input: expand(os.path.join(NEO4J_IMPORTDIR,NODEDIR,'{node}','{node}.csv.gz'), node = config[NODEDIR])
    output: os.path.join(NEO4J_IMPORTDIR,NODEDIR,'created.txt')
    shell: "echo `date` > {NEO4J_IMPORTDIR}/{NODEDIR}/created.txt"

rule process_nodes:
    """
    Process each node
    """
    log: os.path.join(SNAKEMAKELOGS,NODEDIR,'{node}.out'),
    output: os.path.join(NEO4J_IMPORTDIR,NODEDIR,'{node}','{node}.csv.gz')
    params:
        metaData = lambda wildcards: config[NODEDIR][wildcards.node],
        meta_id = lambda wildcards: wildcards.node,
        PROCESSINGDIR=PROCESSINGDIR
    shell: 
        """
        #make neo4j directory
        d={NEO4J_IMPORTDIR}/{NODEDIR}/{params.meta_id}
        mkdir -p $d
        
        #clean up any old import and constraint data
        rm -f $d/{params.meta_id}-import-nodes.txt
        rm -f $d/{params.meta_id}-constraint.txt

        #run the processing script
        python -m {params.PROCESSINGDIR}.{params.metaData[script]} -n {params.meta_id} > {log} 2> {log}
        """

rule read_config_rels:
    """
    Get the rel information from the config file
    """
    #threads: workflow.cores * 0.75
    input: expand(os.path.join(NEO4J_IMPORTDIR,RELDIR,'{rel}','{rel}.csv.gz'), rel = config[RELDIR])
    output: os.path.join(NEO4J_IMPORTDIR,RELDIR,'created.txt')
    shell: "echo `date` > {NEO4J_IMPORTDIR}/{RELDIR}/created.txt"

rule process_rels:
    """
    Process each rel
    """
    log: os.path.join(SNAKEMAKELOGS,NODEDIR,'{rel}.out')
    output: os.path.join(NEO4J_IMPORTDIR,RELDIR,'{rel}','{rel}.csv.gz')
    params:
        metaData = lambda wildcards: config[RELDIR][wildcards.rel],
        meta_id = lambda wildcards: wildcards.rel,
        PROCESSINGDIR=PROCESSINGDIR
    shell: 
        """
        #make directory
        d={NEO4J_IMPORTDIR}/{RELDIR}/{params.meta_id}
        mkdir -p $d
        
        #clean up any old import and constraint data
        rm -f $d/{params.meta_id}-import-rels.txt
        rm -f $d/{params.meta_id}-constraint.txt
        
        #run the processing script
        python -m {params.PROCESSINGDIR}.{params.metaData[script]} -n {params.meta_id} > /dev/null 2> {log}
        """

rule backup_graph:
    log: f"{SNAKEMAKELOGS}/create_graph_backup.log"
    shell: 
        """
        python -m workflow.scripts.graph_build.create_neo4j_backup > {log} 2> {log}
        """